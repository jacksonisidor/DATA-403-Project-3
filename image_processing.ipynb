{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing\n",
    "\n",
    "Turning the images into a usable dataset for a CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Image Transformation\n",
    "\n",
    "*Manually, with a little bit of pytorch*\n",
    "\n",
    "**Input:** Any raw image object (PNG)\n",
    "\n",
    "**Steps:**\n",
    "1. Make sure the image is RGB\n",
    "2. Resize to a consistent shape (224x224 pixels)\n",
    "3. Scale raw pixel values from 0-255 to 0-1\n",
    "4. Convert to tensor\n",
    "    - 3D matrix (3 x Height x Width)\n",
    "    - Each of the 3 layers corresponds to the R, G, or B pixel intensities\n",
    "5. Normalize pixel values based on ImageNet means and stds\n",
    "    - Pretrained CNNs (like ResNet) were trained on the ImageNet dataset\n",
    "    - We need to standardize our pixels to match the ImageNet distribution so the pretrained model gets what it expects.\n",
    "\n",
    "\n",
    "**Output**: An image tensor with the shape (3, 224, 224)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take an image object and return a tensor suitable for model input\n",
    "def image_to_tensor(image):\n",
    "\n",
    "    # convert to RGB\n",
    "    image = image.convert('RGB')\n",
    "    \n",
    "    # rest of preprocessing pipeline\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), # resize to 224x224 pixels\n",
    "        transforms.ToTensor(), # convert to tensor (automatically scales pixel values to 0-1)\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], # normalize with ImageNet RGB distribution\n",
    "                             std=[0.229, 0.224, 0.225]) \n",
    "    ])\n",
    "\n",
    "    # apply preprocessing\n",
    "    image_tensor = preprocess(image)\n",
    "\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test with one image\n",
    "alex1_image = Image.open('Data/Alex/Alex-Image01.png')\n",
    "alex1_tensor = image_to_tensor(alex1_image)\n",
    "alex1_tensor.shape  # should be torch.Size([3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3652,  0.3309,  0.3481,  ...,  0.3652,  0.3309,  0.3481],\n",
       "         [ 0.2796,  0.2796,  0.3652,  ...,  0.4337,  0.3823,  0.3652],\n",
       "         [ 0.3994,  0.3994,  0.4337,  ...,  0.4851,  0.4508,  0.4851],\n",
       "         ...,\n",
       "         [-0.2856, -0.3369, -0.3198,  ...,  0.1768,  0.1254,  0.1426],\n",
       "         [-0.3541, -0.3883, -0.4054,  ...,  0.0227,  0.0227,  0.0912],\n",
       "         [-0.4911, -0.4397, -0.3027,  ..., -0.1143,  0.0741,  0.2282]],\n",
       "\n",
       "        [[ 0.2752,  0.2577,  0.2577,  ...,  0.2927,  0.2752,  0.2927],\n",
       "         [ 0.2227,  0.2227,  0.2752,  ...,  0.3803,  0.3102,  0.3102],\n",
       "         [ 0.3277,  0.3277,  0.3452,  ...,  0.4153,  0.3803,  0.4153],\n",
       "         ...,\n",
       "         [-0.4426, -0.4951, -0.4601,  ...,  0.1176,  0.0826,  0.0826],\n",
       "         [-0.4776, -0.5126, -0.5126,  ..., -0.0399, -0.0224,  0.0126],\n",
       "         [-0.6001, -0.5476, -0.4251,  ..., -0.1275,  0.0476,  0.1702]],\n",
       "\n",
       "        [[ 0.5311,  0.5136,  0.5136,  ...,  0.6356,  0.6182,  0.6182],\n",
       "         [ 0.5659,  0.5659,  0.6008,  ...,  0.6705,  0.6182,  0.6182],\n",
       "         [ 0.6008,  0.6182,  0.6356,  ...,  0.6879,  0.6356,  0.6879],\n",
       "         ...,\n",
       "         [-0.2010, -0.2358, -0.1835,  ...,  0.4614,  0.3742,  0.3742],\n",
       "         [-0.2010, -0.2184, -0.2184,  ...,  0.3219,  0.3393,  0.3219],\n",
       "         [-0.3055, -0.2532, -0.1487,  ...,  0.2696,  0.4265,  0.4788]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex1_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing All Images\n",
    "\n",
    "*Still mostly manual*\n",
    "\n",
    "**Input:** All raw Alex and Kelly images\n",
    "\n",
    "**Steps:**\n",
    "1. Create a tensor for each image in each folder\n",
    "2. Assign a true label (Alex=0, Kelly=1) based on the folder the image came from\n",
    "3. Stack the image tensors together\n",
    "    - 4D matrix (N x 3 x height x width)\n",
    "    - N = # of images = ~400\n",
    "4. Stack the labels together\n",
    "    - 1D matrix\n",
    "\n",
    "**Output:** \n",
    "- X: a stack of image tensors of shape (N, 3, 244, 244) \n",
    "    - X[i] = ith processed image\n",
    "- y: a stack of labels of shape (N)\n",
    "    - y[i] = label for image i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take many image objects and return a full 4D tensor\n",
    "def all_images_to_tensor(images):\n",
    "    tensor_list = [image_to_tensor(img) for img in images]\n",
    "    batch_tensor = torch.stack(tensor_list)\n",
    "    return batch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 224, 224])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on a few images\n",
    "images = [\n",
    "    Image.open('Data/Alex/Alex-Image01.png'),\n",
    "    Image.open('Data/Alex/Alex-Image02.png'),\n",
    "    Image.open('Data/Alex/Alex-Image03.png')\n",
    "]\n",
    "batch_tensor = all_images_to_tensor(images)\n",
    "batch_tensor.shape  # should be torch.Size([3, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go through the Alex and Kelly folders to create one full tensor while assigning true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([485, 3, 224, 224])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "# loop through Alex images\n",
    "for filename in os.listdir('Data/Alex'):\n",
    "    img = Image.open(os.path.join('Data/Alex', filename))\n",
    "    images.append(img)\n",
    "    labels.append(0) # Alex = 0\n",
    "\n",
    "# loop through Kelly images\n",
    "for filename in os.listdir('Data/Kelly'):\n",
    "    img = Image.open(os.path.join('Data/Kelly', filename))\n",
    "    images.append(img)\n",
    "    labels.append(1) # Kelly = 1\n",
    "\n",
    "# convert all images to a single tensor\n",
    "full_tensor = all_images_to_tensor(images)\n",
    "full_tensor.shape  # should be torch.Size([total_images, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full PyTorch Transformation\n",
    "\n",
    "You can also just have PyTorch do the whole thing. Advantages of this:\n",
    "- Very short code, ImageFolder and DataLoader do almost all the steps\n",
    "- The outputted dataset is very accessible and formatted nicely\n",
    "- dataset/dataloader only loads images on demand so the entire tensor is not stored in memory at all times\n",
    "    - not as big of an issue with only ~400 images, but still nice\n",
    "\n",
    "So use this for future modeling, but the more manual steps above are nice for outlining the actual process, or if we need specific control over a step for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# have PyTorch handle the rest\n",
    "dataset = datasets.ImageFolder(root='Data', transform=transform)\n",
    "dataloader = DataLoader(dataset, shuffle=True)\n",
    "len(dataset) # should be the total number of images in both folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things you can do with `dataset`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look at the classes (should be the names of the folders, Alex and Kelly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alex', 'Kelly']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The classes as indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alex': 0, 'Kelly': 1}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract the tensor and label from a single image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape: torch.Size([3, 224, 224])\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "tensor, label = dataset[0]\n",
    "print(f\"Tensor shape: {tensor.shape}\") # should be torch.Size([3, 224, 224])\n",
    "print(f\"Label: {label}\") # should be 0 or 1 depending on the image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
